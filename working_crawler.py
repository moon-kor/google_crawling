# file_checker.py - íŒŒì¼ ìƒíƒœ í™•ì¸ ë° ë³µêµ¬

import os
import shutil

def check_current_file():
    """í˜„ì¬ íŒŒì¼ ìƒíƒœ í™•ì¸"""
    print("ğŸ” í˜„ì¬ google_news_cron.py íŒŒì¼ ìƒíƒœ í™•ì¸")
    print("-" * 50)
    
    try:
        with open('google_news_cron.py', 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.split('\n')
        print(f"ğŸ“„ íŒŒì¼ ì½ê¸° ì„±ê³µ (ì´ {len(lines)}ì¤„)")
        
        # ë¬¸ì œê°€ ìˆëŠ” ì¤„ ì°¾ê¸°
        problem_lines = []
        for i, line in enumerate(lines, 1):
            line_stripped = line.strip()
            # í•¨ìˆ˜/í´ë˜ìŠ¤ ë°–ì— ìˆìœ¼ë©´ ì•ˆ ë˜ëŠ” ì½”ë“œë“¤
            problematic_patterns = [
                'for i, data in enumerate(datas)',
                'data[\'content\']',
                'self.dbManager'
            ]
            
            for pattern in problematic_patterns:
                if pattern in line_stripped and not line_stripped.startswith('#'):
                    # ì´ ì¤„ì´ í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                    indent_level = len(line) - len(line.lstrip())
                    if indent_level < 4:  # í´ë˜ìŠ¤/í•¨ìˆ˜ ë°–ì— ìˆìŒ
                        problem_lines.append((i, line_stripped, pattern))
        
        if problem_lines:
            print("âŒ ë¬¸ì œ ë°œê²¬:")
            for line_num, line_content, pattern in problem_lines:
                print(f"   ì¤„ {line_num}: {line_content[:60]}...")
                print(f"   ë¬¸ì œ: '{pattern}'ì´ í•¨ìˆ˜ ë°–ì— ìˆìŒ")
            return False
        else:
            print("âœ… ê¸°ë³¸ êµ¬ì¡°ëŠ” ì •ìƒì…ë‹ˆë‹¤")
            return True
            
    except FileNotFoundError:
        print("âŒ google_news_cron.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return False

def find_backup_files():
    """ë°±ì—… íŒŒì¼ ì°¾ê¸°"""
    print("\nğŸ” ë°±ì—… íŒŒì¼ ì°¾ê¸°")
    print("-" * 30)
    
    backup_files = []
    for file in os.listdir('.'):
        if file.startswith('google_news_cron_backup') and file.endswith('.py'):
            backup_files.append(file)
    
    if backup_files:
        backup_files.sort(reverse=True)  # ìµœì‹ ìˆœ
        print(f"ğŸ“ ë°œê²¬ëœ ë°±ì—… íŒŒì¼: {len(backup_files)}ê°œ")
        for i, backup in enumerate(backup_files, 1):
            print(f"   {i}. {backup}")
        return backup_files
    else:
        print("âŒ ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return []

def restore_from_backup():
    """ë°±ì—…ì—ì„œ ë³µêµ¬"""
    backup_files = find_backup_files()
    
    if not backup_files:
        return False
    
    print(f"\nê°€ì¥ ìµœì‹  ë°±ì—…ìœ¼ë¡œ ë³µêµ¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print(f"íŒŒì¼: {backup_files[0]}")
    
    choice = input("ë³µêµ¬í•˜ë ¤ë©´ 'y', ì·¨ì†Œí•˜ë ¤ë©´ 'n': ").lower()
    
    if choice == 'y':
        try:
            shutil.copy2(backup_files[0], 'google_news_cron.py')
            print(f"âœ… {backup_files[0]}ì—ì„œ ë³µêµ¬ ì™„ë£Œ!")
            return True
        except Exception as e:
            print(f"âŒ ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return False
    else:
        print("âŒ ë³µêµ¬ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤")
        return False

def create_clean_version():
    """ê¹¨ë—í•œ ìƒˆ ë²„ì „ ìƒì„±"""
    print("\nğŸ”§ ê¹¨ë—í•œ ìƒˆ ë²„ì „ ìƒì„±")
    print("-" * 30)
    
    clean_code = '''from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.jobstores.base import JobLookupError
import requests
import datetime
import maya
import feedparser
from bs4 import BeautifulSoup
import time
import random

import google_news_dbmanager

class GoogleNewsCron():
    def __init__(self):
        print ('í¬ë¡  ì‹œì‘')
        self.scheduler = BackgroundScheduler(job_defaults={'max_instances': 10, 'coalesce': False})
        self.scheduler.start()
        self.dbManager = google_news_dbmanager.GoogleNewsDBManager()

    def __del__(self): 
        self.stop()

    def get_content(self, url):
        """ë‰´ìŠ¤ ë‚´ìš© í¬ë¡¤ë§ í•¨ìˆ˜"""
        print(f"   ğŸ“„ ë‚´ìš© í¬ë¡¤ë§: {url[:50]}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        try:
            time.sleep(random.uniform(1, 2))
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                selectors = [
                    '#dic_area',        # ë„¤ì´ë²„ ë‰´ìŠ¤
                    '.article_view',    # ë‹¤ìŒ ë‰´ìŠ¤
                    '.article-body',    # ì¼ë°˜ì ì¸ íŒ¨í„´
                    '.article_body',    # ë³€í˜•
                    'article',          # HTML5 article íƒœê·¸
                    '.content',         # ì¼ë°˜ì ì¸ content í´ë˜ìŠ¤
                    '.post-content',    # ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼
                    '.entry-content'    # ì›Œë“œí”„ë ˆìŠ¤
                ]
                
                for selector in selectors:
                    element = soup.select_one(selector)
                    if element:
                        content = element.get_text(strip=True)
                        if len(content) > 100:
                            print(f"   âœ… ì„±ê³µ! ê¸¸ì´: {len(content)}ì")
                            return content[:1500]
                
                paragraphs = soup.find_all('p')
                if paragraphs:
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs])
                    if len(content) > 100:
                        print(f"   âœ… píƒœê·¸ë¡œ ì„±ê³µ! ê¸¸ì´: {len(content)}ì")
                        return content[:1500]
                
                print("   âŒ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                return "ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"
                
            else:
                print(f"   âŒ HTTP {response.status_code}")
                return "ì ‘ê·¼ ì‹¤íŒ¨"
                
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {str(e)[:30]}")
            return "í¬ë¡¤ë§ ì˜¤ë¥˜"

    def exec(self, country, keyword):
        print ('Google News Cron Start: ' + datetime.datetime.now().strftime("%m/%d/%Y, %H:%M:%S"))
        URL = 'https://news.google.com/rss/search?q={}+when:1d'.format(keyword)
        if country == 'en':
            URL += '&hl=en-NG&gl=NG&ceid=NG:en'
        elif country == 'ko':
            URL += '&hl=ko&gl=KR&ceid=KR:ko'

        try: 
            res = requests.get(URL)
            if res.status_code == 200:
                datas = feedparser.parse(res.text).entries
                print(f"ğŸ“° ì´ {len(datas)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                
                for i, data in enumerate(datas):
                    print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(datas)} - {data.title}")
                    data['published'] = maya.parse(data.published).datetime(to_timezone="Asia/Seoul", naive=True) 
                    data['source'] = data.source.title
                    data['content'] = self.get_content(data.link)
                    self.dbManager.queryInsertGoogleNewsTable(data)
            else:
                print ('Google ê²€ìƒ‰ ì—ëŸ¬')
        except requests.exceptions.RequestException as err:
            print ('Error Requests: {}'.format(err))
    
    def run(self, mode, country, keyword):
        print ("ì‹¤í–‰!")
        self.dbManager.queryCreateGoogleNewsTable(keyword)
        self.dbManager.queryCreateKeywordTable()
        self.dbManager.queryInsertKeywordTable({
            'keyword': keyword,
            'country': country
        })
        if mode == 'once':
            self.scheduler.add_job(self.exec, args=[country, keyword])
        elif mode == 'interval':
            self.scheduler.add_job(self.exec, 'interval', seconds=10, args=[country, keyword])
        elif mode == 'cron':
            self.scheduler.add_job(self.exec, 'cron', second='*/10', args=[country, keyword])

    def stop(self):
        try: self.scheduler.shutdown() 
        except: pass
        try: self.dbManager.close() 
        except: pass
'''
    
    # í˜„ì¬ íŒŒì¼ ë°±ì—…
    if os.path.exists('google_news_cron.py'):
        backup_name = f'google_news_cron_broken_{datetime.now().strftime("%Y%m%d_%H%M%S")}.py'
        shutil.copy2('google_news_cron.py', backup_name)
        print(f"ğŸ“ í˜„ì¬ íŒŒì¼ ë°±ì—…: {backup_name}")
    
    # ìƒˆ íŒŒì¼ ì‘ì„±
    with open('google_news_cron.py', 'w', encoding='utf-8') as f:
        f.write(clean_code)
    
    print("âœ… ê¹¨ë—í•œ ìƒˆ ë²„ì „ ìƒì„± ì™„ë£Œ!")
    return True

def main():
    print("ğŸ› ï¸ íŒŒì¼ ë³µêµ¬ ë„êµ¬")
    print("=" * 40)
    
    # í˜„ì¬ íŒŒì¼ ìƒíƒœ í™•ì¸
    file_ok = check_current_file()
    
    if not file_ok:
        print("\nğŸ”§ ë³µêµ¬ ì˜µì…˜:")
        print("1. ë°±ì—…ì—ì„œ ë³µêµ¬")
        print("2. ê¹¨ë—í•œ ìƒˆ ë²„ì „ ìƒì„±")
        print("3. working_crawler.py ì‚¬ìš© (ì¶”ì²œ)")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
        
        if choice == '1':
            restore_from_backup()
        elif choice == '2':
            create_clean_version()
        elif choice == '3':
            print("ğŸ’¡ working_crawler.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("   python working_crawler.py")
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤")
    else:
        print("âœ… íŒŒì¼ ìƒíƒœê°€ ì •ìƒì…ë‹ˆë‹¤")

if __name__ == "__main__":
    from datetime import datetime
    main()