# improved_google_news_cron.py
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.jobstores.base import JobLookupError
import requests
import datetime
import maya
import feedparser
from bs4 import BeautifulSoup
import time
import random

import google_news_dbmanager

class ImprovedGoogleNewsCron():
    def __init__(self):
        print('í¬ë¡  ì‹œì‘')
        self.scheduler = BackgroundScheduler(job_defaults={'max_instances': 10, 'coalesce': False})
        self.scheduler.start()
        self.dbManager = google_news_dbmanager.GoogleNewsDBManager()
        
        # ì›¹ì‚¬ì´íŠ¸ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def __del__(self): 
        self.stop()

    def get_article_content(self, url):
        """
        ë‰´ìŠ¤ ë§í¬ì—ì„œ ì‹¤ì œ ë‚´ìš©ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜
        """
        try:
            # ìš”ì²­ ê°„ê²©ì„ ë‘ì–´ ì„œë²„ ë¶€í•˜ ë°©ì§€
            time.sleep(random.uniform(1, 3))
            
            response = requests.get(url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ì—¬ëŸ¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ë³¸ë¬¸ ì¶”ì¶œ íŒ¨í„´
                content_selectors = [
                    'article',  # ì¼ë°˜ì ì¸ article íƒœê·¸
                    '.article-body',  # í´ë˜ìŠ¤ëª…ì´ article-bodyì¸ ìš”ì†Œ
                    '.content',  # í´ë˜ìŠ¤ëª…ì´ contentì¸ ìš”ì†Œ
                    '#article-body',  # IDê°€ article-bodyì¸ ìš”ì†Œ
                    '.post-content',  # ë¸”ë¡œê·¸ í˜•íƒœ
                    'div[data-module="ArticleBody"]',  # íŠ¹ì • ì†ì„±ì„ ê°€ì§„ div
                    '.story-body',  # BBC ìŠ¤íƒ€ì¼
                    '.entry-content'  # ì›Œë“œí”„ë ˆìŠ¤ ìŠ¤íƒ€ì¼
                ]
                
                # ê° ì„ íƒìë¥¼ ì‹œë„í•´ì„œ ë‚´ìš© ì°¾ê¸°
                for selector in content_selectors:
                    content_element = soup.select_one(selector)
                    if content_element:
                        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê³  ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
                        content = content_element.get_text(strip=True)
                        # ë‚´ìš©ì´ ì¶©ë¶„íˆ ê¸¸ë©´ ë°˜í™˜ (ìµœì†Œ 100ì)
                        if len(content) > 100:
                            return content[:2000]  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
                
                # íŠ¹ì • ì„ íƒìë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš°, p íƒœê·¸ë“¤ì„ ëª¨ì•„ì„œ ì‹œë„
                paragraphs = soup.find_all('p')
                if paragraphs:
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs])
                    if len(content) > 100:
                        return content[:2000]
                
                return "ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"ë‚´ìš© í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")
            return "ë‚´ìš© í¬ë¡¤ë§ ì‹¤íŒ¨"

    def exec(self, country, keyword):
        print('Google News Cron Start: ' + datetime.datetime.now().strftime("%m/%d/%Y, %H:%M:%S"))
        URL = 'https://news.google.com/rss/search?q={}+when:1d'.format(keyword)
        
        if country == 'en':
            URL += '&hl=en-NG&gl=NG&ceid=NG:en'
        elif country == 'ko':
            URL += '&hl=ko&gl=KR&ceid=KR:ko'

        try: 
            res = requests.get(URL)
            if res.status_code == 200:
                datas = feedparser.parse(res.text).entries
                
                print(f"ì´ {len(datas)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                
                for i, data in enumerate(datas):
                    print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(datas)} - {data.title}")
                    
                    # ê¸°ì¡´ ë°ì´í„° ì²˜ë¦¬
                    data['published'] = maya.parse(data.published).datetime(to_timezone="Asia/Seoul", naive=True) 
                    data['source'] = data.source.title
                    
                    # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: ë‰´ìŠ¤ ë‚´ìš© í¬ë¡¤ë§
                    article_content = self.get_article_content(data.link)
                    data['content'] = article_content
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë‚´ìš© í¬í•¨)
                    self.dbManager.queryInsertGoogleNewsTable(data)
                    
            else:
                print('Google ê²€ìƒ‰ ì—ëŸ¬')
                
        except requests.exceptions.RequestException as err:
            print('Error Requests: {}'.format(err))
    
    def run(self, mode, country, keyword):
        print("ì‹¤í–‰!")
        self.dbManager.queryCreateGoogleNewsTable(keyword)
        self.dbManager.queryCreateKeywordTable()
        self.dbManager.queryInsertKeywordTable({
            'keyword': keyword,
            'country': country
        })
        
        if mode == 'once':
            self.scheduler.add_job(self.exec, args=[country, keyword])
        elif mode == 'interval':
            self.scheduler.add_job(self.exec, 'interval', seconds=10, args=[country, keyword])
        elif mode == 'cron':
            self.scheduler.add_job(self.exec, 'cron', second='*/10', args=[country, keyword])

    def stop(self):
        try: 
            self.scheduler.shutdown() 
        except: 
            pass
        try: 
            self.dbManager.close() 
        except: 
            pass

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    crawler = ImprovedGoogleNewsCron()
    
    # í•œ ë²ˆë§Œ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
    crawler.run('once', 'ko', 'ì‚¼ì„±ì „ì')
    
    # í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì§€ ì•Šë„ë¡ ëŒ€ê¸°
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        crawler.stop()