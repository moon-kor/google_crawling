from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.jobstores.base import JobLookupError
import requests
import datetime
import maya
import feedparser
from bs4 import BeautifulSoup
import time
import random

import google_news_dbmanager

class GoogleNewsCron():
    def __init__(self):
        print ('í¬ë¡  ì‹œì‘')
        self.scheduler = BackgroundScheduler(job_defaults={'max_instances': 10, 'coalesce': False})
        self.scheduler.start()
        self.dbManager = google_news_dbmanager.GoogleNewsDBManager()

    def __del__(self): 
        self.stop()

    def get_content(self, url):
        """ë‰´ìŠ¤ ë‚´ìš© í¬ë¡¤ë§ í•¨ìˆ˜"""
        print(f"   ğŸ“„ ë‚´ìš© í¬ë¡¤ë§: {url[:50]}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        try:
            time.sleep(random.uniform(1, 2))
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                selectors = [
                    '#dic_area',        # ë„¤ì´ë²„ ë‰´ìŠ¤
                    '.article_view',    # ë‹¤ìŒ ë‰´ìŠ¤
                    '.article-body',    # ì¼ë°˜ì ì¸ íŒ¨í„´
                    '.article_body',    # ë³€í˜•
                    'article',          # HTML5 article íƒœê·¸
                    '.content',         # ì¼ë°˜ì ì¸ content í´ë˜ìŠ¤
                    '.post-content',    # ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼
                    '.entry-content'    # ì›Œë“œí”„ë ˆìŠ¤
                ]
                
                for selector in selectors:
                    element = soup.select_one(selector)
                    if element:
                        content = element.get_text(strip=True)
                        if len(content) > 100:
                            print(f"   âœ… ì„±ê³µ! ê¸¸ì´: {len(content)}ì")
                            return content[:1500]
                
                paragraphs = soup.find_all('p')
                if paragraphs:
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs])
                    if len(content) > 100:
                        print(f"   âœ… píƒœê·¸ë¡œ ì„±ê³µ! ê¸¸ì´: {len(content)}ì")
                        return content[:1500]
                
                print("   âŒ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                return "ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"
                
            else:
                print(f"   âŒ HTTP {response.status_code}")
                return "ì ‘ê·¼ ì‹¤íŒ¨"
                
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {str(e)[:30]}")
            return "í¬ë¡¤ë§ ì˜¤ë¥˜"

    def exec(self, country, keyword):
        print ('Google News Cron Start: ' + datetime.datetime.now().strftime("%m/%d/%Y, %H:%M:%S"))
        URL = 'https://news.google.com/rss/search?q={}+when:1d'.format(keyword)
        if country == 'en':
            URL += '&hl=en-NG&gl=NG&ceid=NG:en'
        elif country == 'ko':
            URL += '&hl=ko&gl=KR&ceid=KR:ko'

        try: 
            res = requests.get(URL)
            if res.status_code == 200:
                datas = feedparser.parse(res.text).entries
                print(f"ğŸ“° ì´ {len(datas)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                
                for i, data in enumerate(datas):
                    print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(datas)} - {data.title}")
                    data['published'] = maya.parse(data.published).datetime(to_timezone="Asia/Seoul", naive=True) 
                    data['source'] = data.source.title
                    data['content'] = self.get_content(data.link)
                    self.dbManager.queryInsertGoogleNewsTable(data)
            else:
                print ('Google ê²€ìƒ‰ ì—ëŸ¬')
        except requests.exceptions.RequestException as err:
            print ('Error Requests: {}'.format(err))
    
    def run(self, mode, country, keyword):
        print ("ì‹¤í–‰!")
        self.dbManager.queryCreateGoogleNewsTable(keyword)
        self.dbManager.queryCreateKeywordTable()
        self.dbManager.queryInsertKeywordTable({
            'keyword': keyword,
            'country': country
        })
        if mode == 'once':
            self.scheduler.add_job(self.exec, args=[country, keyword])
        elif mode == 'interval':
            self.scheduler.add_job(self.exec, 'interval', seconds=10, args=[country, keyword])
        elif mode == 'cron':
            self.scheduler.add_job(self.exec, 'cron', second='*/10', args=[country, keyword])

    def stop(self):
        try: self.scheduler.shutdown() 
        except: pass
        try: self.dbManager.close() 
        except: pass
